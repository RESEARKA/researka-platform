import { createLogger, LogCategory } from './logger';
import * as mammoth from 'mammoth';
import * as iconv from 'iconv-lite';
import { createDeepSeekAI } from './deepseekAI';

// We need to import pdfjs in a way that works with Next.js
let pdfjsLib: any;
if (typeof window !== 'undefined') {
  // Client-side only
  pdfjsLib = require('pdfjs-dist/build/pdf');
  pdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;
}

const logger = createLogger('DocumentParser');

export interface ParsedDocument {
  title?: string;
  abstract?: string;
  keywords?: string[];
  content?: string;
  introduction?: string;
  methods?: string;
  results?: string;
  discussion?: string;
  references?: string[];
  warnings?: string[];
  error?: string;
}

/**
 * Configuration constants for document parsing
 */
const DOCUMENT_PARSING_CONFIG = {
  MAX_ABSTRACT_LINES: 15,
  MIN_ABSTRACT_LENGTH: 100,
  MAX_TITLE_LENGTH: 100,
  MAX_KEYWORDS: 10,
  AI_ENHANCEMENT_ENABLED: true,
};

/**
 * Extracts text content from various document formats
 * Supports: Plain text, PDF, Word documents, and Apple Pages files
 */
export async function parseDocument(file: File, options: { enhanceWithAI?: boolean } = {}): Promise<ParsedDocument> {
  try {
    logger.info(`Parsing document: ${file.name} (${file.type})`, {
      category: LogCategory.DOCUMENT
    });

    // Determine the file type and call the appropriate parser
    let parsedDocument: ParsedDocument;

    if (file.type === 'text/plain' || file.name.endsWith('.txt')) {
      parsedDocument = await parseTextFile(file);
    } else if (file.type === 'application/pdf' || file.name.endsWith('.pdf')) {
      parsedDocument = await parsePdfFile(file);
    } else if (
      file.type === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' ||
      file.name.endsWith('.docx')
    ) {
      parsedDocument = await parseWordFile(file);
    } else if (
      file.type === 'application/msword' ||
      file.name.endsWith('.doc')
    ) {
      parsedDocument = await parseWordFile(file);
    } else if (
      file.type === 'application/vnd.apple.pages' ||
      file.name.endsWith('.pages')
    ) {
      parsedDocument = await parsePagesFile(file);
    } else {
      return {
        error: `Unsupported file type: ${file.type}. Please upload a PDF, Word, Pages, or plain text file.`
      };
    }

    // If AI enhancement is requested and the document was parsed successfully
    if (options.enhanceWithAI && !parsedDocument.error) {
      return await enhanceDocumentWithAI(parsedDocument);
    }

    return parsedDocument;
  } catch (error) {
    logger.error(`Error parsing document: ${error}`, {
      category: LogCategory.DOCUMENT
    });

    return {
      error: `Failed to parse document: ${error instanceof Error ? error.message : String(error)}`
    };
  }
}

/**
 * Parse PDF files using PDF.js
 */
async function parsePdfFile(file: File): Promise<ParsedDocument> {
  if (typeof window === 'undefined') {
    return { error: 'PDF parsing is only available in browser environments' };
  }
  
  try {
    // Get PDF document
    const loadingTask = pdfjsLib.getDocument(new Uint8Array(await file.arrayBuffer()));
    const pdf = await loadingTask.promise;
    
    // Extract text from all pages with improved formatting
    let fullText = '';
    let warnings: string[] = [];
    
    // First pass: extract all text with page numbers for debugging
    const pageTexts: string[] = [];
    
    for (let i = 1; i <= pdf.numPages; i++) {
      const page = await pdf.getPage(i);
      const content = await page.getTextContent();
      
      // Improved text extraction with position awareness
      const textItems = content.items.map((item: any) => ({
        text: item.str,
        x: item.transform[4], // x position
        y: item.transform[5], // y position
        fontName: item.fontName
      }));
      
      // Sort by y position (top to bottom) and then by x position (left to right)
      textItems.sort((a: any, b: any) => {
        // Group items by line (similar y values)
        const yDiff = Math.abs(a.y - b.y);
        if (yDiff < 5) { // Items on the same line
          return a.x - b.x; // Sort by x position
        }
        return b.y - a.y; // Sort by y position (top to bottom)
      });
      
      // Reconstruct lines with proper spacing
      let lastY = -1;
      let lineText = '';
      const lines: string[] = [];
      
      for (const item of textItems) {
        if (lastY !== -1 && Math.abs(item.y - lastY) > 5) {
          // New line
          if (lineText.trim()) {
            lines.push(lineText.trim());
          }
          lineText = item.text;
        } else {
          // Same line or first item
          if (lineText && item.text) {
            // Add space between words if needed
            lineText += ' ' + item.text;
          } else {
            lineText += item.text;
          }
        }
        lastY = item.y;
      }
      
      // Add the last line
      if (lineText.trim()) {
        lines.push(lineText.trim());
      }
      
      const pageText = lines.join('\n');
      pageTexts.push(pageText);
      fullText += pageText + '\n\n'; // Add double newline between pages
    }
    
    // Split text into lines
    const lines = fullText.split(/\r?\n/).filter(line => line.trim());
    
    // Extract title (look for large font or first lines)
    let title = '';
    let abstractStart = 0;
    
    // First, look for title in the first 20 lines
    for (let i = 0; i < Math.min(20, lines.length); i++) {
      const line = lines[i].trim();
      
      // Skip journal name, date, DOI, etc.
      if (line.includes('doi:') || line.includes('DOI:') || 
          line.includes('http') || line.includes('www.') ||
          line.includes('Received:') || line.includes('Accepted:') ||
          line.includes('Published:') || line.includes('Citation:') ||
          line.includes('ISSN') || line.includes('Volume') ||
          line.toLowerCase().includes('journal') || line.includes('©')) {
        continue;
      }
      
      // Likely a title if it's not too long and not too short
      if (line.length > 10 && line.length < 200 && 
          !line.toLowerCase().includes('abstract') && 
          !line.toLowerCase().includes('introduction')) {
        title = line;
        abstractStart = i + 1;
        break;
      }
    }
    
    // Extract abstract
    let abstract = '';
    let contentStart = abstractStart;
    
    // Look for abstract section with multiple patterns
    const abstractPatterns = [
      /abstract/i,
      /summary/i,
      /synopsis/i
    ];
    
    const abstractIndex = lines.findIndex((line, index) => {
      // Only check in the first part of the document
      if (index > 50) return false;
      
      const lineText = line.toLowerCase().trim();
      return abstractPatterns.some(pattern => pattern.test(lineText)) && 
             lineText.length < 30; // Abstract header is usually short
    });
    
    if (abstractIndex !== -1 && abstractIndex < 50) {
      // Start from the line after "Abstract"
      let abstractLines: string[] = [];
      for (let i = abstractIndex + 1; i < lines.length; i++) {
        const line = lines[i].trim();
        
        // Stop at next section header or empty line after collecting some text
        if (abstractLines.length > 3 && 
            (line.toLowerCase().includes('introduction') || 
             line.toLowerCase().includes('keywords') ||
             line.toLowerCase().includes('background') ||
             line.toLowerCase().includes('methods') ||
             (line.match(/^\d+\./) && line.length < 50))) { // Numbered sections like "1. Introduction"
          contentStart = i;
          break;
        }
        
        if (line) {
          abstractLines.push(line);
        }
        
        // If we have collected a reasonable amount of text and hit an empty line, that might be the end of abstract
        if (abstractLines.length >= 5 && !line) {
          contentStart = i + 1;
          break;
        }
      }
      abstract = abstractLines.join(' ');
    } else {
      // No explicit abstract section, take first substantive paragraph
      let abstractLines: string[] = [];
      let foundSubstantiveText = false;
      
      for (let i = abstractStart; i < Math.min(abstractStart + 30, lines.length); i++) {
        const line = lines[i].trim();
        
        // Skip headers, metadata, etc.
        if (!foundSubstantiveText && 
            (line.includes('doi:') || line.includes('DOI:') || 
             line.includes('http') || line.includes('www.') ||
             line.includes('Received:') || line.includes('Accepted:') ||
             line.includes('Published:') || line.includes('Citation:') ||
             line.includes('ISSN') || line.includes('Volume') ||
             line.toLowerCase().includes('journal') || line.includes('©'))) {
          continue;
        }
        
        // Found substantive text
        if (line.length > 50) {
          foundSubstantiveText = true;
        }
        
        if (foundSubstantiveText) {
          // Stop at next section header
          if (line.toLowerCase().includes('introduction') || 
              line.toLowerCase().includes('methods') ||
              line.toLowerCase().includes('background') ||
              (line.match(/^\d+\./) && line.length < 50)) {
            contentStart = i;
            break;
          }
          
          if (line) {
            abstractLines.push(line);
          }
          
          // If we have 5+ lines and hit an empty line, that might be the end of abstract
          if (abstractLines.length >= 5 && !line) {
            contentStart = i + 1;
            break;
          }
        }
      }
      
      if (abstractLines.length > 0) {
        abstract = abstractLines.join(' ');
        warnings.push("No explicit 'Abstract' section found. Using the first substantive paragraph as abstract.");
      } else {
        abstract = '';
        warnings.push("Could not identify an abstract section.");
      }
    }
    
    // Extract keywords
    let keywords: string[] = [];
    
    // Look for keywords section with multiple patterns
    const keywordsPatterns = [
      /keywords/i,
      /key\s+words/i,
      /index\s+terms/i
    ];
    
    const keywordsIndex = lines.findIndex((line, index) => {
      // Only check in the first part of the document
      if (index > 100) return false;
      
      const lineText = line.toLowerCase().trim();
      return keywordsPatterns.some(pattern => pattern.test(lineText)) && 
             lineText.length < 30; // Keywords header is usually short
    });
    
    if (keywordsIndex !== -1 && keywordsIndex < 100) {
      // Extract keywords from the line after the "Keywords" header
      let keywordsLine = '';
      
      // Sometimes keywords are on the same line after a colon
      if (lines[keywordsIndex].includes(':')) {
        keywordsLine = lines[keywordsIndex].split(':')[1].trim();
      } 
      // Otherwise check the next line
      else if (keywordsIndex + 1 < lines.length) {
        keywordsLine = lines[keywordsIndex + 1].trim();
      }
      
      if (keywordsLine) {
        // Try different delimiters: comma, semicolon, or "and"
        if (keywordsLine.includes(';')) {
          keywords = keywordsLine.split(';').map(k => k.trim()).filter(Boolean);
        } else if (keywordsLine.includes(',')) {
          keywords = keywordsLine.split(',').map(k => k.trim()).filter(Boolean);
        } else {
          // Split by "and" or just spaces if no other delimiter is found
          keywords = keywordsLine.split(/\s+and\s+|\s+/).map(k => k.trim()).filter(Boolean);
        }
      }
    }
    
    // Extract document sections
    const sections = extractDocumentSections(lines, contentStart);
    
    // Ensure we have content even if section extraction fails
    if (!sections.introduction && !sections.methods && 
        !sections.results && !sections.discussion) {
      warnings.push("Could not identify standard academic paper sections. The document may have a different structure.");
    }
    
    // Always set the full content for fallback
    const content = lines.slice(contentStart).join('\n');
    
    // Debug logging for section extraction
    console.log('Extracted sections:', JSON.stringify({
      title,
      abstract,
      introduction: sections.introduction || 'Not found',
      methods: sections.methods || 'Not found',
      results: sections.results || 'Not found',
      discussion: sections.discussion || 'Not found',
      references: Array.isArray(sections.references) ? `${sections.references.length} references found` : 'Not found'
    }, null, 2));
    
    return {
      title,
      abstract,
      keywords,
      content,
      ...sections,
      warnings
    };
  } catch (error) {
    console.error('Error parsing PDF:', error);
    return {
      error: error instanceof Error ? error.message : 'Unknown error parsing PDF',
      content: 'Failed to extract content from PDF'
    };
  }
}

/**
 * Parse Word documents using mammoth.js
 */
async function parseWordFile(file: File): Promise<ParsedDocument> {
  try {
    // Convert file to ArrayBuffer
    const arrayBuffer = await file.arrayBuffer();
    
    // Use mammoth to extract text and basic formatting
    const result = await mammoth.extractRawText({ arrayBuffer });
    const text = result.value;
    
    // Split content by lines
    const lines = text.split(/\r?\n/);
    
    // Extract title (first non-empty line)
    let title = '';
    let abstractStart = 0;
    
    for (let i = 0; i < lines.length; i++) {
      if (lines[i].trim()) {
        title = lines[i].trim();
        abstractStart = i + 1;
        break;
      }
    }
    
    // Extract abstract
    let abstractLines: string[] = [];
    let contentStart = abstractStart;
    const warnings: string[] = [];
    
    // Look for abstract section
    const abstractIndex = lines.findIndex(line => 
      line.toLowerCase().includes('abstract') && line.trim().length < 20);
    
    if (abstractIndex !== -1 && abstractIndex < 20) {
      // Start from the line after "Abstract"
      for (let i = abstractIndex + 1; i < lines.length; i++) {
        const line = lines[i].trim();
        
        // Stop at next section header or empty line after collecting some text
        if ((abstractLines.length > 3 && !line) || 
            (line.toLowerCase().includes('introduction') || 
             line.toLowerCase().includes('keywords'))) {
          contentStart = i;
          break;
        }
        
        if (line) {
          abstractLines.push(line);
        }
      }
    } else {
      // No explicit abstract section, take first paragraph
      for (let i = abstractStart; i < Math.min(abstractStart + 15, lines.length); i++) {
        const line = lines[i].trim();
        
        if (i > abstractStart + 3 && 
            (line.toLowerCase().includes('introduction') || 
             line.toLowerCase().includes('methods'))) {
          contentStart = i;
          break;
        }
        
        if (line) {
          abstractLines.push(line);
        }
        
        // If we have 5+ lines and hit an empty line, that might be the end of abstract
        if (abstractLines.length >= 5 && !line) {
          contentStart = i + 1;
          break;
        }
      }
      warnings.push("No explicit 'Abstract' section found. Using the first paragraph as abstract.");
    }
    
    // Extract keywords
    let keywords = extractKeywords(text);
    
    // Look specifically for keywords section
    const keywordsIndex = lines.findIndex(line => 
      line.toLowerCase().includes('keywords') && line.trim().length < 20);
    
    if (keywordsIndex !== -1 && keywordsIndex < 30) {
      // Extract keywords from the line after the "Keywords" header
      const keywordsLine = lines[keywordsIndex + 1].trim();
      if (keywordsLine) {
        keywords = keywordsLine.split(/[,;]/).map(k => k.trim()).filter(Boolean);
      }
    } else {
      warnings.push("No explicit 'Keywords' section found. Keywords were automatically extracted from content.");
    }
    
    // Extract sections for academic papers and whitepapers
    const sectionsMap = extractDocumentSections(lines, contentStart);
    
    // Debug logging for section extraction
    console.log('Extracted sections:', JSON.stringify({
      title,
      abstract: abstractLines.join(' '),
      introduction: sectionsMap.introduction || 'Not found',
      methods: sectionsMap.methods || 'Not found',
      results: sectionsMap.results || 'Not found',
      discussion: sectionsMap.discussion || 'Not found',
      references: Array.isArray(sectionsMap.references) ? `${sectionsMap.references.length} references found` : 'Not found'
    }, null, 2));
    
    // Join the remaining lines as content
    const contentText = lines.slice(contentStart).join('\n');
    
    if (!sectionsMap.introduction && !sectionsMap.methods && !sectionsMap.results && !sectionsMap.discussion) {
      warnings.push("Standard academic sections (Introduction, Methods, Results, Discussion) were not found. Content has been extracted but may need manual organization.");
    }
    
    if (!sectionsMap.references || sectionsMap.references.length === 0) {
      warnings.push("No references section found in the document. Please add references manually if needed.");
    }
    
    return {
      title,
      abstract: abstractLines.join(' '),
      keywords,
      content: contentText,
      introduction: sectionsMap.introduction,
      methods: sectionsMap.methods,
      results: sectionsMap.results,
      discussion: sectionsMap.discussion,
      references: sectionsMap.references,
      warnings
    };
  } catch (error) {
    logger.error(`Error parsing Word document: ${error}`, {
      category: LogCategory.DOCUMENT
    });
    return {
      error: `Failed to parse Word document: ${error instanceof Error ? error.message : String(error)}`
    };
  }
}

/**
 * Extract document sections based on common academic paper structure
 * Improved to handle various section naming conventions and formats
 */
function extractDocumentSections(lines: string[], startIndex: number): Record<string, any> {
  // Initialize sections with empty values
  const sections: Record<string, any> = {
    introduction: '',
    methods: '',
    results: '',
    discussion: '',
    references: [] // Initialize references as an empty array
  };
  
  // Track current section and content
  let currentSection: string | null = null;
  let sectionContent: string[] = [];
  
  // Track if we've found each section
  const foundSections: Record<string, boolean> = {
    introduction: false,
    methods: false,
    results: false,
    discussion: false,
    references: false
  };
  
  // Process each line to identify sections and their content
  for (let i = startIndex; i < lines.length; i++) {
    const line = lines[i].trim();
    if (!line) continue; // Skip empty lines
    
    // Check for section headers using simple pattern matching
    const lowerLine = line.toLowerCase();
    
    // Introduction section patterns
    if (lowerLine === 'introduction' || 
        lowerLine.startsWith('1. introduction') || 
        lowerLine.startsWith('1.introduction') ||
        lowerLine.startsWith('i. introduction') ||
        lowerLine === '1 introduction') {
      
      if (currentSection) {
        // Save content of previous section
        if (currentSection !== 'references') {
          sections[currentSection] = sectionContent.join('\n');
        }
      }
      
      currentSection = 'introduction';
      foundSections.introduction = true;
      sectionContent = [];
      continue;
    }
    
    // Methods section patterns
    if (lowerLine === 'methods' || 
        lowerLine === 'materials and methods' ||
        lowerLine.startsWith('2. ') && (lowerLine.includes('method') || lowerLine.includes('material')) ||
        lowerLine.startsWith('ii. ') && (lowerLine.includes('method') || lowerLine.includes('material')) ||
        lowerLine === '2 methods' ||
        lowerLine === 'methodology') {
      
      if (currentSection) {
        // Save content of previous section
        if (currentSection !== 'references') {
          sections[currentSection] = sectionContent.join('\n');
        }
      }
      
      currentSection = 'methods';
      foundSections.methods = true;
      sectionContent = [];
      continue;
    }
    
    // Results section patterns
    if (lowerLine === 'results' || 
        lowerLine.startsWith('3. results') || 
        lowerLine.startsWith('3.results') ||
        lowerLine.startsWith('iii. results') ||
        lowerLine === '3 results' ||
        lowerLine === 'findings') {
      
      if (currentSection) {
        // Save content of previous section
        if (currentSection !== 'references') {
          sections[currentSection] = sectionContent.join('\n');
        }
      }
      
      currentSection = 'results';
      foundSections.results = true;
      sectionContent = [];
      continue;
    }
    
    // Discussion section patterns
    if (lowerLine === 'discussion' || 
        lowerLine.startsWith('4. discussion') || 
        lowerLine.startsWith('4.discussion') ||
        lowerLine.startsWith('iv. discussion') ||
        lowerLine === '4 discussion' ||
        lowerLine === 'interpretation' ||
        lowerLine === 'discussion and conclusion') {
      
      if (currentSection) {
        // Save content of previous section
        if (currentSection !== 'references') {
          sections[currentSection] = sectionContent.join('\n');
        }
      }
      
      currentSection = 'discussion';
      foundSections.discussion = true;
      sectionContent = [];
      continue;
    }
    
    // References section patterns
    if (lowerLine === 'references' || 
        lowerLine === 'bibliography' ||
        lowerLine === 'works cited' ||
        lowerLine.startsWith('references') ||
        lowerLine === 'literature cited') {
      
      if (currentSection) {
        // Save content of previous section
        if (currentSection !== 'references') {
          sections[currentSection] = sectionContent.join('\n');
        }
      }
      
      currentSection = 'references';
      foundSections.references = true;
      sectionContent = [];
      continue;
    }
    
    // Add line to current section content if we're in a section
    if (currentSection) {
      if (currentSection === 'references') {
        // Handle references as a special case
        if (line.match(/^\[\d+\]/) || line.match(/^\d+\./) || line.match(/^[A-Z][a-z]+,/)) {
          // This looks like a new reference entry
          sections.references.push(line);
        } else if (sections.references.length > 0) {
          // Append to the last reference if it's a continuation
          sections.references[sections.references.length - 1] += ' ' + line;
        } else {
          // First reference doesn't match our patterns, add it anyway
          sections.references.push(line);
        }
      } else {
        // For other sections, just add the line to the content
        sectionContent.push(line);
      }
    }
  }
  
  // Save the last section if there is one
  if (currentSection && currentSection !== 'references') {
    sections[currentSection] = sectionContent.join('\n');
  }
  
  // If we didn't find any sections, try a more aggressive approach
  if (!foundSections.introduction && !foundSections.methods && 
      !foundSections.results && !foundSections.discussion) {
    
    // Try to find sections based on content patterns
    let introStart = -1, methodsStart = -1, resultsStart = -1, discussionStart = -1, referencesStart = -1;
    
    for (let i = startIndex; i < lines.length; i++) {
      const line = lines[i].trim().toLowerCase();
      
      if (line.includes('introduction') && introStart === -1) {
        introStart = i;
      } else if ((line.includes('method') || line.includes('materials')) && methodsStart === -1) {
        methodsStart = i;
      } else if (line.includes('result') && resultsStart === -1) {
        resultsStart = i;
      } else if (line.includes('discussion') && discussionStart === -1) {
        discussionStart = i;
      } else if (line.includes('reference') && referencesStart === -1) {
        referencesStart = i;
      }
    }
    
    // Extract sections based on found indices
    if (introStart !== -1) {
      const endIndex = methodsStart !== -1 ? methodsStart : 
                       resultsStart !== -1 ? resultsStart : 
                       discussionStart !== -1 ? discussionStart : 
                       referencesStart !== -1 ? referencesStart : lines.length;
      sections.introduction = lines.slice(introStart + 1, endIndex).join('\n');
      foundSections.introduction = true;
    }
    
    if (methodsStart !== -1) {
      const endIndex = resultsStart !== -1 ? resultsStart : 
                       discussionStart !== -1 ? discussionStart : 
                       referencesStart !== -1 ? referencesStart : lines.length;
      sections.methods = lines.slice(methodsStart + 1, endIndex).join('\n');
      foundSections.methods = true;
    }
    
    if (resultsStart !== -1) {
      const endIndex = discussionStart !== -1 ? discussionStart : 
                       referencesStart !== -1 ? referencesStart : lines.length;
      sections.results = lines.slice(resultsStart + 1, endIndex).join('\n');
      foundSections.results = true;
    }
    
    if (discussionStart !== -1) {
      const endIndex = referencesStart !== -1 ? referencesStart : lines.length;
      sections.discussion = lines.slice(discussionStart + 1, endIndex).join('\n');
      foundSections.discussion = true;
    }
    
    if (referencesStart !== -1) {
      const referenceLines = lines.slice(referencesStart + 1);
      sections.references = processReferences(referenceLines);
      foundSections.references = true;
    }
  }
  
  // Debug logging for section extraction
  console.log('Section extraction results:', {
    foundIntroduction: foundSections.introduction,
    foundMethods: foundSections.methods,
    foundResults: foundSections.results,
    foundDiscussion: foundSections.discussion,
    foundReferences: foundSections.references,
    introLength: sections.introduction ? sections.introduction.length : 0,
    methodsLength: sections.methods ? sections.methods.length : 0,
    resultsLength: sections.results ? sections.results.length : 0,
    discussionLength: sections.discussion ? sections.discussion.length : 0,
    referencesCount: Array.isArray(sections.references) ? sections.references.length : 0
  });
  
  return sections;
}

/**
 * Process reference lines into individual reference entries
 */
function processReferences(lines: string[]): string[] {
  const references: string[] = [];
  let currentRef = '';
  
  for (const line of lines) {
    if (!line.trim()) continue;
    
    // Check if this is a new reference entry
    if (line.match(/^\[\d+\]/) || line.match(/^\d+\./) || 
        line.match(/^[A-Z][a-z]+,/) || 
        (references.length > 0 && line.match(/^[A-Z]/))) {
      
      if (currentRef) {
        references.push(currentRef);
      }
      currentRef = line;
    } else if (currentRef) {
      // Append to current reference
      currentRef += ' ' + line;
    } else {
      currentRef = line;
    }
  }
  
  // Add the last reference
  if (currentRef) {
    references.push(currentRef);
  }
  
  return references;
}

/**
 * Parse Apple Pages files
 * Note: Pages files are actually zip archives containing XML
 * This is a simplified approach that treats Pages files as XML/text
 */
async function parsePagesFile(file: File): Promise<ParsedDocument> {
  try {
    // For Pages files, we'll try to extract text content
    // Pages files are actually zip archives with XML content
    
    // Read the file as binary data
    const arrayBuffer = await file.arrayBuffer();
    
    // First, check if this is actually a document and not something else (like an image)
    // This is a simple heuristic - we check for common XML tags in Pages documents
    const textSample = iconv.decode(Buffer.from(new Uint8Array(arrayBuffer.slice(0, 1000))), 'utf-8');
    const isLikelyDocument = textSample.includes('<?xml') || 
                             textSample.includes('<document') || 
                             textSample.includes('<iWork') ||
                             textSample.includes('<sl:document');
    
    if (!isLikelyDocument) {
      return {
        error: 'The uploaded file does not appear to be a valid document. Please check the file and try again.'
      };
    }
    
    // Convert to UTF-8 text
    const text = iconv.decode(Buffer.from(new Uint8Array(arrayBuffer)), 'utf-8');
    
    // Look for text content between XML tags
    const textContent = extractTextFromXml(text);
    
    if (!textContent) {
      return {
        error: 'Could not extract text from Pages document. Please export as PDF or Word and try again.'
      };
    }
    
    // Check if the content looks like a table or structured data
    const hasTableStructure = detectTableStructure(textContent);
    
    if (hasTableStructure) {
      // For table-like content, we'll use a different approach
      return {
        title: file.name.replace('.pages', '').replace(/^\d+\s+/, ''), // Remove numbers at the start
        abstract: 'This document contains tabular data that has been extracted. Please review the content carefully.',
        keywords: extractKeywordsFromFileName(file.name),
        content: formatTableContent(textContent)
      };
    }
    
    // Process the extracted text similarly to plain text files
    const lines = textContent.split(/\r?\n/);
    
    // Extract title (first non-empty line)
    let title = '';
    let abstractStart = 0;
    
    for (let i = 0; i < lines.length; i++) {
      if (lines[i].trim()) {
        title = lines[i].trim();
        abstractStart = i + 1;
        break;
      }
    }
    
    // If title looks like binary data or is very long, use the filename instead
    if (title.length > 100 || /[^\x20-\x7E]/.test(title)) {
      title = file.name.replace('.pages', '').replace(/^\d+\s+/, ''); // Remove numbers at the start
    }
    
    // Extract abstract (next few lines)
    let abstractLines: string[] = [];
    let contentStart = abstractStart;
    
    for (let i = abstractStart; i < Math.min(abstractStart + 15, lines.length); i++) {
      const line = lines[i].trim();
      
      if (i > abstractStart + 3 && 
          (line.toLowerCase().includes('introduction') || 
           line.toLowerCase().includes('methods'))) {
        contentStart = i;
        break;
      }
      
      if (line) {
        abstractLines.push(line);
      }
      
      // If we have 5+ lines and hit an empty line, that might be the end of abstract
      if (abstractLines.length >= 5 && !line) {
        contentStart = i + 1;
        break;
      }
    }
    
    // If abstract looks like binary data, provide a generic abstract
    const abstract = abstractLines.join(' ');
    const cleanAbstract = /[^\x20-\x7E]/.test(abstract) || abstract.length > 500 
      ? 'This document was imported from Apple Pages format. Please review the content carefully.'
      : abstract;
    
    // Extract keywords
    const keywords = extractKeywords(textContent);
    
    // Join the remaining lines as content
    const contentText = lines.slice(contentStart).join('\n');
    
    // Clean up content if it appears to be binary or non-text data
    const cleanContent = /[^\x20-\x7E\s]/.test(contentText) || contentText.length > 10000
      ? 'The document content could not be properly extracted. Please consider exporting your Pages document to PDF or Word format for better compatibility.'
      : contentText;
    
    return {
      title,
      abstract: cleanAbstract,
      keywords,
      content: cleanContent
    };
  } catch (error) {
    logger.error(`Error parsing Pages document: ${error}`, {
      category: LogCategory.DOCUMENT
    });
    return {
      error: `Failed to parse Pages document: ${error instanceof Error ? error.message : String(error)}. Pages files are best supported when exported as PDF or Word.`
    };
  }
}

/**
 * Helper function to detect if content appears to be in a table structure
 */
function detectTableStructure(text: string): boolean {
  // Look for patterns that suggest tabular data
  const tableIndicators = [
    // Multiple tab characters
    text.split('\t').length > 10,
    
    // Consistent line lengths suggesting columns
    (() => {
      const lines = text.split('\n').filter(line => line.trim().length > 0);
      if (lines.length < 5) return false;
      
      const wordCounts = lines.map(line => line.split(/\s+/).length);
      const uniqueCounts = new Set(wordCounts);
      
      // If most lines have the same number of words, likely a table
      return uniqueCounts.size <= 3 && lines.length > 10;
    })(),
    
    // Common table-related words
    text.toLowerCase().includes('table') && 
    (text.toLowerCase().includes('row') || text.toLowerCase().includes('column')),
    
    // Repeated patterns of numbers and text
    (/\d+\s+\w+\s+\d+\s+\w+/).test(text)
  ];
  
  // If any indicators are true, it might be a table
  return tableIndicators.some(indicator => indicator === true);
}

/**
 * Format table-like content for better display
 */
function formatTableContent(text: string): string {
  // Split into lines
  const lines = text.split('\n')
    .map(line => line.trim())
    .filter(line => line.length > 0);
  
  // Remove duplicate adjacent lines
  const uniqueLines: string[] = [];
  for (let i = 0; i < lines.length; i++) {
    if (i === 0 || lines[i] !== lines[i-1]) {
      uniqueLines.push(lines[i]);
    }
  }
  
  // Format as markdown sections
  let formattedContent = '## Document Content\n\n';
  
  // If it looks like a language learning document (based on the test case)
  if (uniqueLines.some(line => line.includes('Russian') || line.includes('English'))) {
    formattedContent += '### Vocabulary List\n\n';
    
    // Create a markdown table
    formattedContent += '| # | Term | Translation |\n';
    formattedContent += '|---|------|-------------|\n';
    
    // Process each line
    uniqueLines.forEach((line, index) => {
      // Skip header lines
      if (line.toLowerCase().includes('russian') || 
          line.toLowerCase().includes('english') ||
          line.toLowerCase().includes('translation')) {
        return;
      }
      
      // Try to split the line into columns
      const parts = line.split(/\s{2,}|\t/);
      
      if (parts.length >= 2) {
        const num = index + 1;
        const term = parts[0];
        const translation = parts.slice(1).join(' ');
        formattedContent += `| ${num} | ${term} | ${translation} |\n`;
      } else {
        formattedContent += `| ${index + 1} | ${line} | - |\n`;
      }
    });
  } else {
    // Generic table-like content
    formattedContent += uniqueLines.join('\n\n');
  }
  
  return formattedContent;
}

/**
 * Extract potential keywords from filename
 */
function extractKeywordsFromFileName(filename: string): string[] {
  // Remove extension and numbers
  const cleanName = filename
    .replace(/\.\w+$/, '')
    .replace(/^\d+\s+/, '')
    .replace(/[^\w\s]/g, ' ');
  
  // Split into words
  const words = cleanName.split(/\s+/)
    .filter(word => word.length > 3)
    .map(word => word.toLowerCase());
  
  // Return unique words
  return Array.from(new Set(words));
}

/**
 * Helper function to extract text from XML content (for Pages files)
 */
function extractTextFromXml(xmlContent: string): string {
  try {
    // This is a simplified approach to extract text from XML
    // Remove all XML tags and decode entities
    let text = xmlContent.replace(/<[^>]*>/g, ' ');
    
    // Decode common XML entities
    text = text.replace(/&lt;/g, '<')
               .replace(/&gt;/g, '>')
               .replace(/&amp;/g, '&')
               .replace(/&quot;/g, '"')
               .replace(/&apos;/g, "'");
    
    // Clean up whitespace
    text = text.replace(/\s+/g, ' ').trim();
    
    return text;
  } catch (error) {
    logger.error(`Error extracting text from XML: ${error}`, {
      category: LogCategory.DOCUMENT
    });
    return '';
  }
}

/**
 * Parse plain text files
 */
async function parseTextFile(file: File): Promise<ParsedDocument> {
  return new Promise((resolve) => {
    const reader = new FileReader();
    
    reader.onload = (event) => {
      try {
        const content = event.target?.result as string;
        if (!content) {
          resolve({ error: 'Empty file content' });
          return;
        }
        
        // Split content by lines
        const lines = content.split(/\r?\n/);
        
        // Simple heuristic for title: first non-empty line
        let title = '';
        let abstractStart = 0;
        
        // Find title (first non-empty line)
        for (let i = 0; i < lines.length; i++) {
          if (lines[i].trim()) {
            title = lines[i].trim();
            abstractStart = i + 1;
            break;
          }
        }
        
        // Extract abstract (next few lines)
        let abstractLines: string[] = [];
        let contentStart = abstractStart;
        
        for (let i = abstractStart; i < Math.min(abstractStart + 15, lines.length); i++) {
          const line = lines[i].trim();
          
          if (i > abstractStart + 3 && 
              (line.toLowerCase().includes('introduction') || 
               line.toLowerCase().includes('methods'))) {
            contentStart = i;
            break;
          }
          
          if (line) {
            abstractLines.push(line);
          }
          
          // If we have 5+ lines and hit an empty line, that might be the end of abstract
          if (abstractLines.length >= 5 && !line) {
            contentStart = i + 1;
            break;
          }
        }
        
        // Extract keywords
        const keywords = extractKeywords(content);
        
        // Join the remaining lines as content
        const contentText = lines.slice(contentStart).join('\n');
        
        resolve({
          title,
          abstract: abstractLines.join(' '),
          keywords,
          content: contentText
        });
      } catch (error) {
        logger.error(`Error parsing text file: ${error}`, {
          category: LogCategory.DOCUMENT
        });
        resolve({ 
          error: `Failed to parse text file: ${error instanceof Error ? error.message : String(error)}` 
        });
      }
    };
    
    reader.onerror = () => {
      resolve({ error: 'Failed to read file' });
    };
    
    reader.readAsText(file);
  });
}

/**
 * Helper function to extract keywords from text content
 */
function extractKeywords(text: string): string[] {
  // Look for keyword patterns in the text
  const keywordPatterns = [
    /keywords:(.+?)(?:\n|$)/i,
    /key\s*words:(.+?)(?:\n|$)/i,
    /terms:(.+?)(?:\n|$)/i
  ];
  
  for (const pattern of keywordPatterns) {
    const match = text.match(pattern);
    if (match && match[1]) {
      return match[1].split(/[,;]/).map(k => k.trim()).filter(Boolean);
    }
  }
  
  // If no explicit keywords section, try to extract important terms
  // This is a simplified approach - in a production environment,
  // you might want to use NLP techniques for better keyword extraction
  const words = text.toLowerCase()
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(word => word.length > 3);
  
  // Count word frequency
  const wordCount: Record<string, number> = {};
  words.forEach(word => {
    wordCount[word] = (wordCount[word] || 0) + 1;
  });
  
  // Get top 5 most frequent words as keywords
  return Object.entries(wordCount)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 5)
    .map(([word]) => word);
}

/**
 * Enhance a parsed document using AI to improve structure, formatting, and content quality
 * @param parsedDocument The parsed document to enhance
 * @returns A promise that resolves to the enhanced document
 */
export async function enhanceDocumentWithAI(parsedDocument: ParsedDocument): Promise<ParsedDocument> {
  // If there was an error in parsing or AI enhancement is disabled, return the original document
  if (parsedDocument.error || !DOCUMENT_PARSING_CONFIG.AI_ENHANCEMENT_ENABLED) {
    return parsedDocument;
  }

  try {
    logger.info('Enhancing document with AI', {
      category: LogCategory.DOCUMENT
    });

    // Create the DeepSeek AI client
    const deepseekAI = createDeepSeekAI();

    // Prepare the prompt for AI enhancement
    const prompt = `
You are an academic document formatting assistant. Your task is to enhance the structure and quality of the following document sections.
Please analyze and improve the document while maintaining its original meaning and key information.

DOCUMENT:
Title: ${parsedDocument.title || 'No title provided'}

Abstract: ${parsedDocument.abstract || 'No abstract provided'}

Keywords: ${parsedDocument.keywords?.join(', ') || 'No keywords provided'}

Content:
${parsedDocument.content?.substring(0, 4000) || 'No content provided'}

Please provide an enhanced version of this document with the following improvements:
1. Format the title to be clear, concise, and academically appropriate
2. Structure the abstract to follow academic standards (problem, methods, results, conclusion)
3. Identify and suggest the most relevant keywords (max 10)
4. Organize the content into logical sections with appropriate headings
5. Format any references or citations according to academic standards
6. Correct any grammatical or spelling errors
7. Enhance clarity and readability while preserving the original meaning

Return your response as a JSON object with the following structure:
{
  "title": "Enhanced title",
  "abstract": "Enhanced abstract",
  "keywords": ["keyword1", "keyword2", "..."],
  "content": "Enhanced content with proper sections and formatting"
}
`;

    // Call the DeepSeek API
    const response = await deepseekAI.generateContent(prompt, {
      temperature: 0.3, // Lower temperature for more consistent results
      max_tokens: 2000, // Allow enough tokens for comprehensive enhancement
    });

    if (!response.success || !response.text) {
      logger.warn('AI enhancement failed, returning original document', {
        category: LogCategory.DOCUMENT,
        context: { error: response.error }
      });
      return parsedDocument;
    }

    // Extract the enhanced document from the AI response
    try {
      // Helper function from DeepSeekAI to extract JSON from responses
      const extractJsonFromResponse = (text: string): any => {
        try {
          // First try direct parsing
          return JSON.parse(text);
        } catch (e) {
          // Check if the response contains a JSON code block
          const jsonBlockRegex = /```(?:json)?\s*\n([\s\S]*?)\n```/;
          const match = text.match(jsonBlockRegex);
          
          if (match && match[1]) {
            try {
              // Try parsing the content inside the code block
              return JSON.parse(match[1]);
            } catch (innerError) {
              throw new Error(`Failed to parse JSON from code block: ${innerError}`);
            }
          }
          
          // If no code block, try to find anything that looks like JSON
          const jsonRegex = /\{[\s\S]*\}/;
          const jsonMatch = text.match(jsonRegex);
          
          if (jsonMatch) {
            try {
              return JSON.parse(jsonMatch[0]);
            } catch (jsonError) {
              throw new Error(`Failed to parse JSON from response: ${jsonError}`);
            }
          }
          
          throw new Error('No valid JSON found in response');
        }
      };

      const enhancedDocument = extractJsonFromResponse(response.text);

      // Validate the enhanced document structure
      if (!enhancedDocument.title || !enhancedDocument.abstract) {
        logger.warn('AI enhancement returned incomplete document, using partial results', {
          category: LogCategory.DOCUMENT
        });
        
        // Merge the enhanced document with the original, preferring enhanced values
        return {
          title: enhancedDocument.title || parsedDocument.title,
          abstract: enhancedDocument.abstract || parsedDocument.abstract,
          keywords: enhancedDocument.keywords || parsedDocument.keywords,
          content: enhancedDocument.content || parsedDocument.content
        };
      }

      logger.info('Document enhanced successfully with AI', {
        category: LogCategory.DOCUMENT
      });

      // Return the enhanced document
      return {
        title: enhancedDocument.title,
        abstract: enhancedDocument.abstract,
        keywords: enhancedDocument.keywords || parsedDocument.keywords,
        content: enhancedDocument.content || parsedDocument.content
      };
    } catch (jsonError) {
      logger.error(`Error parsing AI enhancement response: ${jsonError}`, {
        category: LogCategory.DOCUMENT
      });
      
      // If we can't parse the JSON, return the original document
      return parsedDocument;
    }
  } catch (error) {
    logger.error(`Error enhancing document with AI: ${error}`, {
      category: LogCategory.DOCUMENT
    });
    
    // Return the original document if enhancement fails
    return parsedDocument;
  }
}
